{
  "stats": {
    "vetted": 4,
    "beingBuilt": 0,
    "launched": 0
  },
  "ideas": [
    {
      "id": 4,
      "url": "https://github.com/shivankar-madaan/notbuiltyet/issues/4",
      "title": "Land Intelligence Engine — Ecological Intervention Scoring for Natural Capital",
      "category": "Environment",
      "categoryClass": "tag-env",
      "description": {
        "problem": "Countries are under pressure to meet \"30x30\" targets (protecting 30% of land by 2030), unlock carbon credits, and hit net-zero commitments — but they...",
        "solution": "A \"Google Maps for Natural Capital\" — a platform that ingests open data sources (satellite imagery, SAR radar, LiDAR, soil databases, climate...",
        "why": "Intelligence-led planting achieves ~80% survival vs 10–20% for random planting — significantly lower cost for the same outcome The platform doesn't..."
      },
      "fullDescription": {
        "problem": "Countries are under pressure to meet \"30x30\" targets (protecting 30% of land by 2030), unlock carbon credits, and hit net-zero commitments — but they lack the tools to prioritize. For any given piece of land, nobody can answer the basic question: \"What is the highest-value ecological intervention here, and what's the ROI?\" Reforestation today is poorly targeted. Organizations plant trees in locations with 10–20% survival rates, using species poorly matched to soil and climate conditions. Carbon credit projects get funded with no forward-looking climate analysis — trees planted today need to survive in a climate that's 1–2°C warmer with different rainfall patterns by 2050, but almost nobody models this. Urban planners don't know which 500 meters of tree canopy would reduce peak electricity load the most. Insurance companies can't quantify how upstream forest restoration reduces downstream flood claims. The data to answer these questions exists — free satellite imagery (Sentinel-2, Landsat), global soil databases (SoilGrids, SSURGO), biodiversity records (GBIF — 2+ billion observations), climate projections (CMIP6), LiDAR terrain data — but it's fragmented across hundreds of institutions in incompatible formats, and nobody has built the intelligence layer that fuses it into actionable decisions.",
        "solution": "A \"Google Maps for Natural Capital\" — a platform that ingests open data sources (satellite imagery, SAR radar, LiDAR, soil databases, climate projections, biodiversity records, land ownership, hydrology, air quality) and produces a comprehensive ecological-economic score for every 10m x 10m pixel of land. The platform runs six core modules: Land Suitability Scoring — For every pixel: slope, soil depth/pH/drainage, water availability, current land cover, contamination risk, legal status, and critically — forward-looking climate suitability for 2050, not just current conditions. Output: a 0–100 suitability score decomposed into sub-scores. Species Recommendation Engine — Multi-objective optimization that recommends not a single tree but an ecological community (canopy species, understory, nitrogen fixers, early colonizers). Species matched to projected 2050 climate envelopes with assisted migration considerations. Uses the TRY Plant Trait Database (12M trait records, 280K species). Ecosystem Services Quantification — For each site: carbon sequestration trajectory with monetary value at current credit prices; water services (groundwater recharge, flood risk reduction, water quality improvement); biodiversity value (threatened species habitat, corridor connectivity); urban-specific services (cooling in avoided kWh, air quality in health cost savings, stormwater management, property value uplift of 5–15% from mature canopy). Risk Assessment — Fire risk, drought mortality, pest/disease, encroachment probability, political/governance risk, and climate change risk to the planting itself. Produces risk-adjusted ROI accounting for probability of failure. Prioritization Engine — Synthesizes everything into ranked opportunities. Clients adjust weighting (water utility weights water, carbon fund weights carbon, city government weights cooling). Flags \"quick wins\": sites where natural regeneration succeeds with just protection, drone seeding is viable, or public land has zero acquisition cost. Monitoring & Verification — Satellite-based survival monitoring post-planting, anomaly detection for underperforming areas, carbon stock updates, and MRV reports formatted for credit standards (Verra VCS, Gold Standard, ART TREES).",
        "why": "Intelligence-led planting achieves ~80% survival vs 10–20% for random planting — significantly lower cost for the same outcome The platform doesn't plant trees — it sells the decision layer showing governments and corporations where to invest, with verified ROI projections High-quality removal credits trade at \\$22–\\$35/tCO2e vs \\$5–\\$10 for unverified avoidance credits — the verification layer alone justifies the platform Urban cooling is quantifiable and cities are starting to pay for avoided peak grid load After 5 years of monitoring, the longitudinal dataset of what works where becomes a strong reference that's hard to replicate Companies are legally required to do environmental assessments — compliance automation is a real market Growing skepticism around \"ghost forests\" means buyers will pay a premium for verified proof that planted trees are alive and sequestering carbon Can start with two people, a laptop, and free satellite data — then scale country by country"
      },
      "improvement": "~4x improvement in tree survival rates (80% vs 10-20%), 90% reduction in site assessment time, ability to quantify ecosystem services (carbon, water, biodiversity, cooling) that are currently unquantified for most land",
      "moats": [
        {
          "name": "Data Moat",
          "cssClass": "moat-data"
        },
        {
          "name": "Technical",
          "cssClass": "moat-technical"
        },
        {
          "name": "Domain Expertise",
          "cssClass": "moat-domain"
        },
        {
          "name": "First Mover",
          "cssClass": "moat-first-mover"
        },
        {
          "name": "Integration Depth",
          "cssClass": "moat-integration"
        }
      ],
      "viability": 82,
      "defensibility": "Very High",
      "votes": 0
    },
    {
      "id": 3,
      "url": "https://github.com/shivankar-madaan/notbuiltyet/issues/3",
      "title": "Deep Sea Species Detection & Environmental Compliance Platform",
      "category": "Environment",
      "categoryClass": "tag-env",
      "description": {
        "problem": "Deep sea expeditions are expensive — a single ROV dive can cost \\$50,000–\\$100,000/day. They generate hours of footage, most of which gets reviewed...",
        "solution": "A real-time vision model running on the ROV itself (or on the ship) that watches the live feed and does three things: Matches known species against a...",
        "why": "Partnering with institutions like NOAA, Schmidt Ocean Institute, or Woods Hole provides a continuous stream of labeled footage — every dive makes the..."
      },
      "fullDescription": {
        "problem": "Deep sea expeditions are expensive — a single ROV dive can cost \\$50,000–\\$100,000/day. They generate hours of footage, most of which gets reviewed by 1–2 marine biologists months or years later. Novel species get spotted, logged in a spreadsheet, and wait years for a taxonomist to officially describe them. Some never get followed up on at all. The ocean below 200m is so poorly explored that scientists estimate we've formally described maybe 10–20% of species living there. Deep sea footage is difficult to work with — poor lighting, particles in the water, motion blur, creatures that are semi-transparent or bioluminescent, and high similarity between related species. Current marine biology databases like OBIS or WoRMS have records but very few standardized images at depth. Training data is scarce and fragmented across dozens of institutions who don't share well.",
        "solution": "A real-time vision model running on the ROV itself (or on the ship) that watches the live feed and does three things: Matches known species against a database of deep sea life with high confidence Flags anomalies — things that don't match anything in the database, ranked by how unusual they are Alerts the operator mid-dive so they can slow down, reposition, collect a sample, or get a better angle before the creature disappears The key insight is that the window to investigate is the dive itself. By the time a human reviews footage back on land, the moment is gone. The \"unknown\" detection problem is harder than standard classification — you're not just asking \"what is this?\" but \"is this something we've never seen?\" That requires anomaly detection, not just a classifier. The business pivot: don't just sell species identification — sell environmental compliance. Offshore wind and oil/gas companies are legally required to do biodiversity assessments before they can drill or build. They have large budgets and hate waiting for manual footage review. Automate their compliance reports and you have a scalable B2B business. As deep sea mining kicks off, environmental impact monitoring will be a significant mandate. The pitch: \"We automate the environmental permitting bottleneck for offshore energy — what currently takes 6 months of manual footage review takes us 48 hours, and our reports are defensible in court.\"",
        "why": "Partnering with institutions like NOAA, Schmidt Ocean Institute, or Woods Hole provides a continuous stream of labeled footage — every dive makes the model better, and after 5 years the dataset becomes very hard to replicate Offshore wind is projected to be a \\$1T industry by 2040 — every installation requires an Environmental Impact Assessment currently done by expensive manual contractors Deep sea organisms are a significant source of novel compounds for pharma — being the default search layer for deep sea biology creates a strong gatekeeper position for bio-prospecting The US Navy has interest in understanding biological activity for distinguishing whale sounds from submarine signatures — defense contracts change the funding trajectory Grows into: citizen science layer on recreational dive footage, passive monitoring buoys, environmental DNA cross-referencing, and eventually a living, continuously updated atlas of ocean biodiversity"
      },
      "improvement": "~90% reduction in footage review time for environmental compliance, real-time anomaly detection vs months-delayed manual review",
      "moats": [
        {
          "name": "Data Moat",
          "cssClass": "moat-data"
        },
        {
          "name": "Technical",
          "cssClass": "moat-technical"
        },
        {
          "name": "Domain Expertise",
          "cssClass": "moat-domain"
        },
        {
          "name": "First Mover",
          "cssClass": "moat-first-mover"
        }
      ],
      "viability": 62,
      "defensibility": "High",
      "votes": 0
    },
    {
      "id": 2,
      "url": "https://github.com/shivankar-madaan/notbuiltyet/issues/2",
      "title": "Asteroid Prospecting Platform — Composition Intelligence from Public Survey Data",
      "category": "Other",
      "categoryClass": "tag-other",
      "description": {
        "problem": "The asteroid mining industry is moving fast — multiple startups are racing to extract resources from near-Earth asteroids. But they all face the same...",
        "solution": "A prospecting platform that fuses public data from NASA (NeoWs, JPL SBDB), ESA, JAXA, GAIA spacecraft, and ground-based surveys to build a...",
        "why": "Asteroid resources represent a large frontier — a single metallic asteroid can contain more platinum-group metals than have ever been mined on Earth...."
      },
      "fullDescription": {
        "problem": "The asteroid mining industry is moving fast — multiple startups are racing to extract resources from near-Earth asteroids. But they all face the same bottleneck: we know where asteroids are (NASA tracks 30,000+ NEOs), but we barely know what they're made of. Spectral data exists for only a fraction of known asteroids, and even when it does, going from \"S-type classification\" to \"estimated platinum content\" is unreliable. The best public tool for this was built in 2012 with basic heuristics and appears unmaintained. Current ML models only achieve ~92% accuracy on broad taxonomic groups and routinely confuse critical categories (C-type vs X-type). Mining companies are making multi-million dollar mission decisions with poor geological intelligence.",
        "solution": "A prospecting platform that fuses public data from NASA (NeoWs, JPL SBDB), ESA, JAXA, GAIA spacecraft, and ground-based surveys to build a comprehensive composition intelligence layer for asteroids. ML models trained on meteorite spectra, orbital family membership, albedo, and multi-parameter constraints to predict composition where spectral data is sparse or missing. Each asteroid gets an economic viability score combining estimated resource value, delta-v accessibility, launch window frequency, rotation rate, and size. Real-time updates as new survey data arrives. Think of it as a Bloomberg Terminal for asteroid mining — the intelligence layer that every mining company needs but none are building as a standalone product.",
        "why": "Asteroid resources represent a large frontier — a single metallic asteroid can contain more platinum-group metals than have ever been mined on Earth. But this industry can't scale on guesswork. Companies launching missions need reliable geological intelligence the same way terrestrial mining companies rely on geological surveys before drilling. By making this intelligence accessible, the platform lowers the cost of bad bets and helps the space mining industry make better decisions. The data is already public and free — the interpretation layer is what's missing."
      },
      "improvement": "~10-100x better composition prediction coverage vs current public tools (existing tools cover 600K asteroids with rough heuristics; ML can provide probabilistic composition estimates for all 30K+ NEOs with confidence scores)",
      "moats": [
        {
          "name": "Data Moat",
          "cssClass": "moat-data"
        },
        {
          "name": "Technical",
          "cssClass": "moat-technical"
        },
        {
          "name": "Domain Expertise",
          "cssClass": "moat-domain"
        },
        {
          "name": "First Mover",
          "cssClass": "moat-first-mover"
        }
      ],
      "viability": 62,
      "defensibility": "High",
      "votes": 0
    },
    {
      "id": 1,
      "url": "https://github.com/shivankar-madaan/notbuiltyet/issues/1",
      "title": "Construction Site Progress Monitoring — Automated Visual Tracking from Drone & Satellite Imagery",
      "category": "Infrastructure",
      "categoryClass": "tag-infra",
      "description": {
        "problem": "Construction is one of the least digitized industries. Projects routinely run over budget and behind schedule — globally, large construction projects...",
        "solution": "A platform that uses drone and satellite imagery combined with computer vision to automatically monitor construction sites — turning raw visual data...",
        "why": "Catches safety violations in near real-time instead of after an accident Makes public infrastructure projects accountable with verifiable, objective..."
      },
      "fullDescription": {
        "problem": "Construction is one of the least digitized industries. Projects routinely run over budget and behind schedule — globally, large construction projects average 80% over budget and 20 months late. The downstream effects are significant: Wasted resources: Delays and rework waste materials, energy, and labor at scale Safety gaps: Construction remains one of the most dangerous industries. Poor site monitoring means safety violations (missing PPE, improper scaffolding, unsecured zones) go undetected until someone gets hurt Accountability gaps: Progress is tracked through subjective site visits and inconsistent photo documentation — there's no reliable, quantifiable way to know if a project is on track Infrastructure delays: Hospitals, schools, housing, and public infrastructure projects get delayed for years, directly impacting the communities waiting for them Fraud and misreporting: Without verifiable progress data, misrepresentation of construction status is hard to detect — especially in public projects and aid-funded developments",
        "solution": "A platform that uses drone and satellite imagery combined with computer vision to automatically monitor construction sites — turning raw visual data into objective, quantifiable progress metrics. How It Works: Data Collection — Drones capture high-resolution aerial footage on a regular schedule. Freely available satellite imagery (Sentinel-2, Landsat) supplements with continuous coverage Visual Analysis — Computer vision models detect and quantify structural progress (foundation, floors, walls), material stockpiles, equipment activity, and safety violations like missing barriers or workers without protective equipment Transparency & Alerts — Automated reports with visual before/after comparisons, real-time dashboards showing objective completion metrics, and immediate alerts when progress deviates from plan or safety issues are detected",
        "why": "Catches safety violations in near real-time instead of after an accident Makes public infrastructure projects accountable with verifiable, objective progress data Catches deviations early when course-correction is still cheap Enables transparency for aid-funded and government projects where misreporting is a real problem Makes project oversight scalable and affordable for developing regions"
      },
      "improvement": "~70% reduction in manual inspection overhead, 3-5x faster deviation and safety violation detection",
      "moats": [
        {
          "name": "Data Moat",
          "cssClass": "moat-data"
        },
        {
          "name": "Technical",
          "cssClass": "moat-technical"
        },
        {
          "name": "Domain Expertise",
          "cssClass": "moat-domain"
        },
        {
          "name": "First Mover",
          "cssClass": "moat-first-mover"
        },
        {
          "name": "Integration Depth",
          "cssClass": "moat-integration"
        }
      ],
      "viability": 82,
      "defensibility": "High",
      "votes": 0
    }
  ]
}
